{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0\"></a>\n",
    "# **K-Means Clustering with Python by Ousmane KA and Albert Sandokh BAKHOUM**\n",
    "\n",
    "The aim of this project is to leverage the **K-Means clustering algorithm**  to analyze patient health data and identify common health profiles based on key attributes such as age, gender, BMI, blood pressure, glucose level, and the presence of chronic conditions like diabetes, stroke, or asthma. The dataset includes diverse patient characteristics, making it ideal for uncovering hidden patterns that can aid in understanding group trends and tailoring healthcare strategies. Specifically, we will explore how to group patients into clusters that reflect similar health traits and determine the optimal number of clusters based on disease prevalence and other relevant characteristics. This approach offers valuable insights for preventative care and resource allocation in healthcare settings.\n",
    "\n",
    "\n",
    "So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "![](https://cdn-images-1.medium.com/max/1200/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "\n",
    "\n",
    "1.\t[Introduction to K-Means Clustering](#1)\n",
    "1.  [Applications of clustering](#2)\n",
    "1.\t[K-Means Clustering intuition](#3)\n",
    "1.\t[Choosing the value of K](#4)\n",
    "1.\t[The elbow method](#5)\n",
    "1.  [Import libraries](#6)\n",
    "1.\t[Import dataset](#7)\n",
    "1.\t[Exploratory data analysis](#8)\n",
    "1.\t[Declare feature vector and target variable](#9)\n",
    "1.\t[Convert categorical variable into integers](#10)\n",
    "1.\t[Feature scaling](#11)\n",
    "1.\t[K-Means model with two clusters](#12)\n",
    "1.\t[K-Means model parameters study](#13)\n",
    "1.\t[Check quality of weak classification by the model](#14)\n",
    "1.\t[Use elbow method to find optimal number of clusters](#15)\n",
    "1.\t[K-Means model with different clusters](#16)\n",
    "1.\t[Results and conclusion](#17)\n",
    "1.  [References](#18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to K-Means Clustering** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "\n",
    "Machine learning algorithms can be broadly classified into two categories - supervised and unsupervised learning. There are other categories also like semi-supervised learning and reinforcement learning. But, most of the algorithms are classified as supervised or unsupervised learning. The difference between them happens because of presence of target variable. In unsupervised learning, there is no target variable. The dataset only has input variables which describe the data. This is called unsupervised learning.\n",
    "\n",
    "**K-Means clustering** is the most popular unsupervised learning algorithm. It is used when we have unlabelled data which is data without defined categories or groups. The algorithm follows an easy or simple way to classify a given data set through a certain number of clusters, fixed apriori. K-Means algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity.\n",
    "\n",
    "K-means clustering is therefore a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:\n",
    "\n",
    "* The centroids of the K clusters, which can be used to label new data\n",
    "* Labels for the training data (each data point is assigned to a single cluster)\n",
    "\n",
    "K-Means clustering can be represented diagrammatically as follows:-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "![K-Means](https://miro.medium.com/max/2160/1*tWaaZX75oumVwBMcKN-eHA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Applications of clustering** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "\n",
    "### Clustering\n",
    "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n",
    "\n",
    "K-Means clustering is the most common unsupervised machine learning algorithm. It is widely used for many applications which include-\n",
    "\n",
    "  1. Image segmentation\n",
    "\n",
    "  2. Customer segmentation\n",
    "\n",
    "  3. Species clustering\n",
    "\n",
    "  4. Anomaly detection\n",
    "\n",
    "  5. Clustering languages\n",
    "\n",
    "### Applications\n",
    "The K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets. Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group.\n",
    "\n",
    "This is a versatile algorithm that can be used for any type of grouping. Some examples of use cases are:\n",
    "\n",
    "* Behavioral segmentation:\n",
    "* * Segment by purchase history\n",
    "* * Segment by activities on application, website, or platform\n",
    "* * Define personas based on interests\n",
    "* * Create profiles based on activity monitoring\n",
    "* Inventory categorization:\n",
    "* * Group inventory by sales activity\n",
    "* * Group inventory by manufacturing metrics\n",
    "* Sorting sensor measurements:\n",
    "* * Detect activity types in motion sensors\n",
    "* * Group images\n",
    "* * Separate audio\n",
    "* * Identify groups in health monitoring\n",
    "* Detecting bots or anomalies:\n",
    "* * Separate valid activity groups from bots\n",
    "  \n",
    "### Uses\n",
    "* **Search engine**: Search engine, groups results together using clustering algorithm\n",
    "* **Customer segmentation**: K-mean clustering can be used to create customer clusters based on demographic information, geographical information and behavioral data.\n",
    "* **Social network analysis**: To find groups of people with specific interest to direct the personalized ads.\n",
    "* **Data center**: To organize the computer clusters in data center.\n",
    "* **Inventory management**: Create inventory clusters based on sales number and manufacturing capacity\n",
    "\n",
    "### Advantages\n",
    "* One of the simplest algorithm to understand\n",
    "* Since it uses simple computations it is relatively efficient\n",
    "* Gives better results when there is less data overlapping\n",
    "\n",
    "### Disadvantages\n",
    "* Number of clusters need to be defined by user\n",
    "* Doesn't work well in case of overlapping data\n",
    "* Unable to handle the noisy data and outliers\n",
    "* Algorithm fails for non-linear data set\n",
    "\n",
    "### KMeans Clustering \n",
    "K-means clustering is one of the simplest and popular unsupervised machine learning algorithms. You’ll define a target number k, which refers to the number of centroids you need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Every data point is allocated to each of the clusters through reducing the in-cluster sum of squares. In other words, the K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible. The ‘means’ in the K-means refers to averaging of the data; that is, finding the centroid.\n",
    "\n",
    "### About the dataset \n",
    "The dataset contains health-related attributes for patients, including demographics (age, gender), clinical measures (BMI, blood pressure, glucose levels), and the presence of chronic conditions (diabetes, stroke, asthma), enabling analysis of health profiles and trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. K-Means Clustering intuition** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "\n",
    "K-Means clustering is used to find intrinsic groups within the unlabelled dataset and draw inferences from them. It is based on centroid-based clustering.\n",
    "\n",
    "\n",
    "**Centroid** - A centroid is a data point at the centre of a cluster. In centroid-based clustering, clusters are represented by a centroid. It is an iterative algorithm in which the notion of similarity is derived by how close a data point is to the centroid of the cluster.\n",
    "K-Means clustering works as follows:-\n",
    "The K-Means clustering algorithm uses an iterative procedure to deliver a final result. The algorithm requires number of clusters K and the data set as input. The data set is a collection of features for each data point. The algorithm starts with initial estimates for the K centroids. The algorithm then iterates between two steps:\n",
    "\n",
    "\n",
    "## **3.1 Data assignment step**\n",
    "\n",
    "\n",
    "Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, which is based on the squared Euclidean distance. So, if ci is the collection of centroids in set C, then each data point is assigned to a cluster based on minimum Euclidean distance. \n",
    "\n",
    "\n",
    "\n",
    "## **3.2 Centroid update step**\n",
    "\n",
    "\n",
    "In this step, the centroids are recomputed and updated. This is done by taking the mean of all data points assigned to that centroid’s cluster. \n",
    "\n",
    "\n",
    "The algorithm then iterates between step 1 and step 2 until a stopping criteria is met. Stopping criteria means no data points change the clusters, the sum of the distances is minimized or some maximum number of iterations is reached.\n",
    "This algorithm is guaranteed to converge to a result. The result may be a local optimum meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n",
    "\n",
    "The K-Means intuition can be represented with the help of following diagram:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means intuition\n",
    "![K-Means intuition](https://i.ytimg.com/vi/_aWzGGNrcic/hqdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid Random Initialisation Trap\n",
    "\n",
    "### Random Initialization Guidelines\n",
    "To avoid random initialization trap, follow below guidelines for random initialization.\n",
    "\n",
    "* Number of cluster centroids should be less than number of training examples\n",
    "* To avoid local optima issue, try to do multiple random initialization of centroids. \n",
    "* Multiple random initialization technique is more effective when we have a small number of clusters.\n",
    "* Similarly for large number of clusters, few random initialization are sufficient\n",
    "\n",
    "Through these images let's see how two different random initialisations can cause a totally different outcome.\n",
    "\n",
    "### Init 1\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/zsC9z0z.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Init 2\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/kU5BX6j.png)\n",
    "\n",
    "So we saw that even with clear distinction possible visually, wrong randomisation can produce wrong results.\n",
    "There have been researches carried out and one of the most famous ways to initialise centroids is KMeans++.\n",
    "The best thing is that the whole algorithm remains the same but the only difference is that we provide an argument to SKlearn to use KMeans++ for initialisation. There are many papers explaining the KMeans++ but the explanation is beyond this notebook for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Choosing the value of K** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "\n",
    "The K-Means algorithm depends upon finding the number of clusters and data labels for a pre-defined value of K. To find the number of clusters in the data, we need to run the K-Means clustering algorithm for different values of K and compare the results. So, the performance of K-Means algorithm depends upon the value of K. We should choose the optimal value of K that gives us best performance. There are different techniques available to find the optimal value of K. The most common technique is the **elbow method** which is described below.\n",
    "\n",
    "The way to evaluate the choice of K is made using a parameter known as WCSS. WCSS stands for **Within Cluster Sum of Squares**.\n",
    "It should be low. Here's the formula representation for example when K = 3\n",
    "\n",
    "Summation Distance(p,c) is the sum of distance of points in a cluster from the centroid.\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/5W63xul.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. The elbow method** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "The Elbow Method is then used to choose the best K value. In the depiction below we can see that after 3 there's no significant decrease in WCSS so 3 is the best here. Therefore there's an elbow shape that forms and it is usually a good idea to pick the number where this elbow is formed. There would be many times when the graph wouldn't be this intuitive but with practice it becomes easier.\n",
    "\n",
    "The elbow method is used to determine the optimal number of clusters in K-means clustering. The elbow method plots the value of the cost function produced by different values of K. The below diagram shows how the elbow method works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The elbow method\n",
    "\n",
    "![Elbow method in K-Means](https://miro.medium.com/v2/resize:fit:670/0*aY163H0kOrBO46S-.png)\n",
    "\n",
    "![](https://i.imgur.com/gi9p7V5.png)\n",
    "\n",
    "![K_Means_Elbow_Method](https://raw.githubusercontent.com/satishgunjal/images/master/K_Means_Elbow_Method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if K increases, average distortion will decrease.  Then each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as K increases. The value of K at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Working Principle**\n",
    "\n",
    "Let's now discuss the working of KMeans algorithm. The aim is to break the explanation down in the simplest way possible. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Inner Working of K-Means Clustering \n",
    "K-means is often referred to as Lloyd’s algorithm. It is one of the most popular clustering algorithm. Refer below plot where there are two clusters (K=2) one is of red data points and another one of green data points.\n",
    "\n",
    "![Two_Cluster_Centroid_With_Datapoints](https://raw.githubusercontent.com/satishgunjal/images/master/Two_Cluster_Centroid_With_Datapoints.png)\n",
    "\n",
    "So how does K-Means algorithm find the clusters of the data points without any label? Below steps will explain the inner working of K-Means algorithm. \n",
    "\n",
    "* First step is to finalize the number of clusters you want to identify in your data. This is the \"K\" in K-means clustering.\n",
    "* Now randomly initialize the points equal to the number of clusters K. 'Cluster Centroid' is the terminology used to refer these points.\n",
    "* Note that centroid means center point of given dataset, but initially these points are at random location, but at the end when K-Means algorithm will converge they will be at the center of their respective cluster.\n",
    "* Once cluster centroids are defined, K-means algorithm will go through each data point from given data and depending on that points closeness to cluster centroid, it will assign the data point to the cluster centroid. This is called as 'Assignment Step'.\n",
    "* In order to move the cluster centroids from random location to their respective group, K-means algorithm will find the mean of each data point assigned to the cluster centroid and move the respective centroid to the mean value location. This is called as 'Move Centroid Step'\n",
    "* Note that during 'Move Centroid Step' data points can get reassigned from one cluster to another as centroid position change.\n",
    "* Now repeat the assignment and move centroid steps till cluster centroid position don't change. K-means algorithm will converge when we get the unchanged position of cluster centroids. \n",
    "* Once K-means algorithm is converged, data point assigned to respective centroid will represent the respective cluster.\n",
    "* During cluster assignment step if we found a centroid who has no data point associated with it, then it's better to remove it.\n",
    "\n",
    "![Inner_Working_K_Means](https://raw.githubusercontent.com/satishgunjal/images/master/Inner_Working_K_Means.png)\n",
    "\n",
    "Since we have to randomly pick the cluster centroids, it's initialization may affect the final outcome of the clustering. In case our initialization is not correct, then K-Means algorithm may form a cluster with few points only. Such situation is referred as 'centroid random initialization trap' and it may cause algorithm to get stuck at local optima.\n",
    "\n",
    "Check below plots, where for same dataset, we end up getting different clusters depending on initial position of cluster centroids. Gray color squares represent the initial positions of centroids and red, green and blue squares represent the final position of centroids.\n",
    "\n",
    "\n",
    "## Working of K-Means Clustering \n",
    "\n",
    "##### It begins with choosing the number of K clusters. The K signifies the number of clusters that the algorithm would find in the dataset. Now choosing the right K is very important. Sometimes the K is clearly visible from the dataset when visualized. However most of the times this is not the case and in a short time we'll see about how to choose the right K value.\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/RBK4dtA.png)\n",
    "\n",
    "##### The second step is to allocate K random points as centroids. These K points could be points from the dataset or outside. There's one thing to note however. The random initialisation of centroids can sometimes cause random initialisation trap which we would see in this section soon.\n",
    "\n",
    "![](https://i.imgur.com/LfI2qfl.png)\n",
    "\n",
    "##### In the third step the dataset points would be allocated to the centroid which is closest to them.\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/9I5JH3m.png)\n",
    "\n",
    "##### The fourth step is to calculate the centroid of the individual clusters and place the old centroid there.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/FyIeKuA.png)\n",
    "\n",
    "##### The fifth step is to reassign points like we did in step 3. If reassignment takes place then we need to go back to step four. If no reassignment takes place then we can say that our model has converged and its ready.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/aRaGcKB.png)\n",
    "\n",
    "## **Step Summary**\n",
    "##### To summarise the steps we can say :\n",
    "![](https://i.imgur.com/3jTk7Y0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Import libraries** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Libraries imports\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import seaborn as sns # for statistical data visualization\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import os\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Import dataset** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Patient_Dataset_KMeans.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Exploratory data analysis** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 7050 instances and 16 attributes in the dataset. In the dataset description, it is given that there are 7051 instances and 12 attributes in the dataset.\n",
    "\n",
    "So, we can infer that the first instance is the row header and there are 4 extra attributes in the dataset. Next, we should take a look at the dataset to gain more insight about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View summary of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 4 redundant columns in the dataset. We should drop them before proceeding further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop redundant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.drop(['Column1', 'Column2', 'Column3', 'Column4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again view summary of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that redundant columns have been removed from the dataset. \n",
    "\n",
    "We can see that, there are 3 character variables (data type = object) and remaining 9 numerical variables (data type = int64).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the statistical summary of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 categorical variables in the dataset. I will explore them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `status_id` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# view the labels in the variable\n",
    "\n",
    "df['status_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# view how many different types of variables are there\n",
    "\n",
    "len(df['status_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 6997 unique labels in the `status_id` variable. The total number of instances in the dataset is 7050. So, it is approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `status_published` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# view the labels in the variable\n",
    "\n",
    "df['status_published'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# view how many different types of variables are there\n",
    "\n",
    "len(df['status_published'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see that there are 6913 unique labels in the `status_published` variable. The total number of instances in the dataset is 7050. So, it is also a approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore `status_type` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# view the labels in the variable\n",
    "\n",
    "df['status_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# view how many different types of variables are there\n",
    "\n",
    "len(df['status_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 4 categories of labels in the `status_type` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop `status_id` and `status_published` variable from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.drop(['status_id', 'status_published'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the summary of dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is 1 non-numeric column `status_type` in the dataset. I will convert it into integer equivalents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. Declare feature vector and target variable** <a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df\n",
    "\n",
    "y = df['status_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10. Convert categorical variable into integers** <a class=\"anchor\" id=\"10\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X['status_type'] = le.fit_transform(X['status_type'])\n",
    "\n",
    "y = le.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the summary of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the dataset X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **11. Feature Scaling** <a class=\"anchor\" id=\"11\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ms = MinMaxScaler()\n",
    "\n",
    "X = ms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **12. K-Means model with two clusters** <a class=\"anchor\" id=\"12\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0) \n",
    "\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **13. K-Means model parameters study** <a class=\"anchor\" id=\"13\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The KMeans algorithm clusters data by trying to separate samples in n groups of equal variances, minimizing a criterion known as **inertia**, or within-cluster sum-of-squares Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are.\n",
    "\n",
    "\n",
    "- The k-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean j of the samples in the cluster. The means are commonly called the cluster **centroids**.\n",
    "\n",
    "\n",
    "- The K-means algorithm aims to choose centroids that minimize the inertia, or within-cluster sum of squared criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inertia\n",
    "\n",
    "\n",
    "- **Inertia** is not a normalized metric. \n",
    "\n",
    "- The lower values of inertia are better and zero is optimal. \n",
    "\n",
    "- But in very high-dimensional spaces, euclidean distances tend to become inflated (this is an instance of `curse of dimensionality`). \n",
    "\n",
    "- Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.\n",
    "\n",
    "- We can calculate model inertia as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lesser the model inertia, the better the model fit.\n",
    "\n",
    "- We can see that the model has very high inertia. So, this is not a good model fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **14. Check quality of weak classification by the model** <a class=\"anchor\" id=\"14\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "\n",
    "# check how many of the samples were correctly labeled\n",
    "correct_labels = sum(y == labels)\n",
    "\n",
    "print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved a weak classification accuracy of 1% by our unsupervised model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **15. Use elbow method to find optimal number of clusters** <a class=\"anchor\" id=\"15\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "cs = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(X)\n",
    "    cs.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('CS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By the above plot, we can see that there is a kink at k=2. \n",
    "\n",
    "- Hence k=2 can be considered a good number of the cluster to cluster this data.\n",
    "\n",
    "- But, we have seen that I have achieved a weak classification accuracy of 1% with k=2.\n",
    "\n",
    "- I will write the required code with k=2 again for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=2,random_state=0)\n",
    "\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# check how many of the samples were correctly labeled\n",
    "\n",
    "correct_labels = sum(y == labels)\n",
    "\n",
    "print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n",
    "\n",
    "print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our weak unsupervised classification model achieved a very weak classification accuracy of 1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will check the model accuracy with different number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **16. K-Means model with different clusters** <a class=\"anchor\" id=\"16\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means model with 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "\n",
    "kmeans.fit(X)\n",
    "\n",
    "# check how many of the samples were correctly labeled\n",
    "labels = kmeans.labels_\n",
    "\n",
    "correct_labels = sum(y == labels)\n",
    "print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n",
    "print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means model with 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "\n",
    "kmeans.fit(X)\n",
    "\n",
    "# check how many of the samples were correctly labeled\n",
    "labels = kmeans.labels_\n",
    "\n",
    "correct_labels = sum(y == labels)\n",
    "print(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n",
    "print('Accuracy score: {0:0.2f}'. format(correct_labels/float(y.size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have achieved a relatively high accuracy of 62% with k=4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **17. Results and conclusion** <a class=\"anchor\" id=\"17\"></a>\n",
    "\n",
    "\n",
    "1.\tIn this project, we have implemented the most popular unsupervised clustering technique called **K-Means Clustering**.\n",
    "\n",
    "2.\tWe have applied the elbow method and find that k=? (k is number of clusters) can be considered a good number of cluster to cluster this data.\n",
    "\n",
    "3.\tWe have find that the model has very high inertia of ?. So, this is not? a good model fit to the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **18. References** <a class=\"anchor\" id=\"18\"></a>\n",
    "\n",
    "The work done in this project is inspired from following books and websites:-\n",
    "\n",
    "  1. Kaggle\n",
    "\n",
    "  1. Udemy course – Machine Learning – A Z by Kirill Eremenko and Hadelin de Ponteves\n",
    "\n",
    "  2. https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "  3. https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "\n",
    "  4. https://www.datacamp.com/community/tutorials/k-means-clustering-python\n",
    "\n",
    "  5. https://www.datascience.com/blog/k-means-clustering\n",
    "\n",
    "  6. https://acadgild.com/blog/k-means-clustering-algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we will come to the end of this kernel.\n",
    "\n",
    "I hope you find this kernel useful and enjoyable.\n",
    "\n",
    "Your comments and feedback are most welcome.\n",
    "\n",
    "Thank you\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 260377,
     "sourceId": 545527,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29849,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
